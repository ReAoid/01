"""
AIå¯¹è¯å¤„ç†æµæ°´çº¿

å®ç°å®Œæ•´çš„AIå¯¹è¯å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬è¾“å…¥é¢„å¤„ç†ã€ä¸Šä¸‹æ–‡ç®¡ç†ã€è®°å¿†æ£€ç´¢ã€
äººè®¾åº”ç”¨ã€AIç”Ÿæˆå’Œåå¤„ç†ç­‰æ­¥éª¤
"""

import asyncio
import json
import re
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, AsyncGenerator
from uuid import UUID

import openai
from openai import AsyncOpenAI

from ..config import get_settings
from ..models.chat_models import ChatMessage, MessageRole, MessageCreate, ConversationContext
from ..models.memory_models import Memory
from .context_manager import ContextManager
from .memory_manager import MemoryManager
from .character_system import CharacterSystem
from .stream_handler import StreamHandler
from ..services.ollama_service import get_ollama_service
from ..utils.logger import get_logger

logger = get_logger(__name__)
settings = get_settings()


class AIPipeline:
    """AIå¯¹è¯å¤„ç†æµæ°´çº¿"""
    
    def __init__(self):
        self.context_manager = ContextManager()
        self.memory_manager = MemoryManager()
        self.character_system = CharacterSystem()
        self.stream_handler = StreamHandler()
        
        # AIå®¢æˆ·ç«¯
        self.ollama_service = None
        self.openai_client = None
        self.anthropic_client = None
        
        self._init_ai_clients_sync()
    
    async def _init_ai_clients(self):
        """åˆå§‹åŒ–AIå®¢æˆ·ç«¯"""
        try:
            # OllamaæœåŠ¡
            self.ollama_service = await get_ollama_service()
            if self.ollama_service and self.ollama_service.available_models:
                logger.info(f"OllamaæœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼Œå¯ç”¨æ¨¡å‹: {len(self.ollama_service.available_models)}ä¸ª")
            else:
                logger.warning("OllamaæœåŠ¡åˆå§‹åŒ–å¤±è´¥æˆ–æ— å¯ç”¨æ¨¡å‹")
            
            # OpenAIå®¢æˆ·ç«¯
            if settings.openai_api_key:
                self.openai_client = AsyncOpenAI(
                    api_key=settings.openai_api_key
                )
                logger.info("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
            # Anthropicå®¢æˆ·ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
            # if settings.anthropic_api_key:
            #     import anthropic
            #     self.anthropic_client = anthropic.AsyncAnthropic(
            #         api_key=settings.anthropic_api_key
            #     )
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–AIå®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    def _init_ai_clients_sync(self):
        """åŒæ­¥åˆå§‹åŒ–AIå®¢æˆ·ç«¯ï¼ˆç”¨äº__init__ï¼‰"""
        try:
            # OpenAIå®¢æˆ·ç«¯
            if settings.openai_api_key:
                self.openai_client = AsyncOpenAI(
                    api_key=settings.openai_api_key
                )
                logger.info("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–AIå®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    async def process_message(
        self,
        user_id: UUID,
        session_id: UUID,
        message: MessageCreate,
        character_id: str = "default",
        stream_id: Optional[str] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯çš„å®Œæ•´æµæ°´çº¿"""
        
        start_time = time.time()
        processing_metadata = {
            "start_time": start_time,
            "steps_completed": [],
            "tokens_used": 0,
            "model_used": None,
            "character_used": character_id,
            "memory_retrieved": [],
            "processing_time": 0
        }
        
        try:
            # ç¬¬1æ­¥ï¼šè¾“å…¥é¢„å¤„ç†ä¸åˆ†æ
            logger.info(f"å¼€å§‹å¤„ç†æ¶ˆæ¯: ç”¨æˆ·{user_id}, ä¼šè¯{session_id}")
            
            processed_input = await self._preprocess_input(message.content)
            processing_metadata["steps_completed"].append("input_preprocessing")
            
            # ç¬¬2æ­¥ï¼šæ„å›¾è¯†åˆ«ä¸å®ä½“æå–
            intent_analysis = await self._analyze_intent(processed_input)
            processing_metadata["intent"] = intent_analysis
            processing_metadata["steps_completed"].append("intent_analysis")
            
            # ç¬¬3æ­¥ï¼šè·å–å†å²æ¶ˆæ¯å’Œä¸Šä¸‹æ–‡
            # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“è·å–ï¼Œæš‚æ—¶ä½¿ç”¨ç©ºåˆ—è¡¨
            historical_messages = []  # await self._get_historical_messages(session_id)
            
            # ç¬¬4æ­¥ï¼šé•¿æœŸè®°å¿†æ£€ç´¢
            relevant_memories = await self.memory_manager.retrieve_relevant_memories(
                user_id=user_id,
                query=processed_input,
                limit=settings.memory.retrieval_max_results
            )
            
            memory_context = self._build_memory_context(relevant_memories)
            processing_metadata["memory_retrieved"] = [str(mem[0].id) for mem in relevant_memories]
            processing_metadata["steps_completed"].append("memory_retrieval")
            
            # ç¬¬5æ­¥ï¼šäººè®¾åº”ç”¨
            character_config = self.character_system.get_character(character_id)
            processing_metadata["steps_completed"].append("character_loading")
            
            # ç¬¬6æ­¥ï¼šä¸Šä¸‹æ–‡æ„å»ºä¸ä¼˜åŒ–
            context = await self.context_manager.build_context(
                session_id=session_id,
                user_input=processed_input,
                messages=historical_messages,
                character_config=character_config,
                memory_context=memory_context
            )
            
            processing_metadata["tokens_used"] = context.total_tokens
            processing_metadata["steps_completed"].append("context_building")
            
            # ç¬¬7æ­¥ï¼šæ„å»ºAIæç¤º
            formatted_messages = self.context_manager.format_context_for_ai(
                context=context,
                character_config=character_config,
                memory_context=memory_context,
                user_input=processed_input
            )
            
            processing_metadata["steps_completed"].append("prompt_engineering")
            
            # ç¬¬8æ­¥ï¼šAIç”Ÿæˆ
            if stream_id and self.stream_handler.is_stream_active(stream_id):
                # æµå¼ç”Ÿæˆ
                response = await self._generate_streaming_response(
                    formatted_messages, character_config, stream_id, processing_metadata
                )
            else:
                # éæµå¼ç”Ÿæˆ
                response = await self._generate_response(
                    formatted_messages, character_config, processing_metadata
                )
            
            processing_metadata["steps_completed"].append("ai_generation")
            
            # ç¬¬9æ­¥ï¼šäººè®¾ä¸€è‡´æ€§æ£€æŸ¥å’Œè°ƒæ•´
            adjusted_response, was_adjusted = await self.character_system.apply_character_consistency(
                response, character_config, historical_messages
            )
            
            if was_adjusted:
                processing_metadata["response_adjusted"] = True
                response = adjusted_response
            
            processing_metadata["steps_completed"].append("consistency_check")
            
            # ç¬¬10æ­¥ï¼šåå¤„ç†ä¸ä¼˜åŒ–
            final_response = await self._postprocess_response(
                response, character_config, intent_analysis
            )
            processing_metadata["steps_completed"].append("postprocessing")
            
            # ç¬¬11æ­¥ï¼šè®°å¿†æ›´æ–°
            await self._update_memories(
                user_id, session_id, processed_input, final_response, context
            )
            processing_metadata["steps_completed"].append("memory_update")
            
            # è®¡ç®—æ€»å¤„ç†æ—¶é—´
            processing_metadata["processing_time"] = time.time() - start_time
            
            logger.info(
                f"æ¶ˆæ¯å¤„ç†å®Œæˆ: {len(processing_metadata['steps_completed'])}ä¸ªæ­¥éª¤, "
                f"è€—æ—¶{processing_metadata['processing_time']:.2f}ç§’"
            )
            
            return final_response, processing_metadata
            
        except Exception as e:
            logger.error(f"å¤„ç†æ¶ˆæ¯å¤±è´¥: {e}")
            processing_metadata["error"] = str(e)
            processing_metadata["processing_time"] = time.time() - start_time
            
            # è¿”å›é”™è¯¯å¤„ç†å“åº”
            error_response = await self._generate_error_response(str(e), character_config)
            return error_response, processing_metadata
    
    async def _preprocess_input(self, user_input: str) -> str:
        """è¾“å…¥é¢„å¤„ç†ä¸æ ‡å‡†åŒ–"""
        try:
            # 1. åŸºç¡€æ¸…ç†
            processed = user_input.strip()
            
            # 2. å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
            processed = re.sub(r'\s+', ' ', processed)
            
            # 3. æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
            punctuation_map = {
                'ï¼': '!',
                'ï¼Ÿ': '?',
                'ï¼Œ': ',',
                'ã€‚': '.',
                'ï¼›': ';',
                'ï¼š': ':',
                '"': '"',
                '"': '"',
                ''': "'",
                ''': "'"
            }
            
            for chinese_punct, english_punct in punctuation_map.items():
                processed = processed.replace(chinese_punct, english_punct)
            
            # 4. å¤„ç†ç‰¹æ®Šå­—ç¬¦
            processed = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:\'\"()[\]{}@#$%^&*+=<>/-]', '', processed)
            
            # 5. é•¿åº¦æ£€æŸ¥
            if len(processed) > 2000:
                processed = processed[:2000]
                logger.warning("è¾“å…¥å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­")
            
            return processed
            
        except Exception as e:
            logger.error(f"è¾“å…¥é¢„å¤„ç†å¤±è´¥: {e}")
            return user_input
    
    async def _analyze_intent(self, processed_input: str) -> Dict[str, Any]:
        """æ„å›¾è¯†åˆ«ä¸å®ä½“æå–"""
        try:
            intent_analysis = {
                "primary_intent": "unknown",
                "confidence": 0.0,
                "entities": [],
                "sentiment": "neutral",
                "urgency": "normal",
                "topic_category": "general"
            }
            
            input_lower = processed_input.lower()
            
            # ç®€å•çš„æ„å›¾è¯†åˆ«
            intent_patterns = {
                "question": ["?", "ï¼Ÿ", "ä»€ä¹ˆ", "æ€ä¹ˆ", "å¦‚ä½•", "ä¸ºä»€ä¹ˆ", "å“ªé‡Œ", "è°", "when", "what", "how", "why"],
                "request": ["è¯·", "å¸®æˆ‘", "èƒ½å¦", "å¯ä»¥", "éœ€è¦", "æƒ³è¦", "å¸Œæœ›"],
                "greeting": ["ä½ å¥½", "hi", "hello", "æ—©ä¸Šå¥½", "ä¸‹åˆå¥½", "æ™šä¸Šå¥½"],
                "farewell": ["å†è§", "æ‹œæ‹œ", "goodbye", "bye", "ç»“æŸ"],
                "complaint": ["é—®é¢˜", "é”™è¯¯", "ä¸å¯¹", "ä¸è¡Œ", "æ•…éšœ", "bug"],
                "praise": ["å¥½", "æ£’", "ä¸é”™", "ä¼˜ç§€", "perfect", "excellent"],
                "casual_chat": ["èŠå¤©", "æ— èŠ", "éšä¾¿", "é—²èŠ"]
            }
            
            max_confidence = 0.0
            detected_intent = "unknown"
            
            for intent, keywords in intent_patterns.items():
                matches = sum(1 for keyword in keywords if keyword in input_lower)
                confidence = matches / len(keywords)
                
                if confidence > max_confidence:
                    max_confidence = confidence
                    detected_intent = intent
            
            intent_analysis["primary_intent"] = detected_intent
            intent_analysis["confidence"] = min(max_confidence * 2, 1.0)  # è°ƒæ•´ç½®ä¿¡åº¦
            
            # æƒ…æ„Ÿåˆ†æ
            positive_words = ["å¥½", "æ£’", "å–œæ¬¢", "æ»¡æ„", "å¼€å¿ƒ", "é«˜å…´", "è°¢è°¢"]
            negative_words = ["ä¸å¥½", "ç³Ÿç³•", "è®¨åŒ", "å¤±æœ›", "éš¾è¿‡", "ç”Ÿæ°”", "é—®é¢˜"]
            
            positive_count = sum(1 for word in positive_words if word in input_lower)
            negative_count = sum(1 for word in negative_words if word in input_lower)
            
            if positive_count > negative_count:
                intent_analysis["sentiment"] = "positive"
            elif negative_count > positive_count:
                intent_analysis["sentiment"] = "negative"
            
            # ç´§æ€¥ç¨‹åº¦åˆ¤æ–­
            urgent_keywords = ["ç´§æ€¥", "æ€¥", "é©¬ä¸Š", "ç«‹å³", "å¿«", "urgent", "asap"]
            if any(keyword in input_lower for keyword in urgent_keywords):
                intent_analysis["urgency"] = "high"
            
            # ä¸»é¢˜åˆ†ç±»
            topic_keywords = {
                "technical": ["ä»£ç ", "ç¼–ç¨‹", "bug", "ç³»ç»Ÿ", "è½¯ä»¶", "æŠ€æœ¯"],
                "business": ["å·¥ä½œ", "é¡¹ç›®", "ä¼šè®®", "å®¢æˆ·", "ä¸šåŠ¡"],
                "personal": ["ç”Ÿæ´»", "å®¶åº­", "æœ‹å‹", "æ„Ÿæƒ…", "å¥åº·"],
                "learning": ["å­¦ä¹ ", "è¯¾ç¨‹", "çŸ¥è¯†", "æ•™ç¨‹", "å­¦ä¼š"]
            }
            
            for topic, keywords in topic_keywords.items():
                if any(keyword in input_lower for keyword in keywords):
                    intent_analysis["topic_category"] = topic
                    break
            
            return intent_analysis
            
        except Exception as e:
            logger.error(f"æ„å›¾åˆ†æå¤±è´¥: {e}")
            return {"primary_intent": "unknown", "confidence": 0.0}
    
    def _build_memory_context(self, relevant_memories: List[Tuple[Memory, float]]) -> str:
        """æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡"""
        if not relevant_memories:
            return ""
        
        try:
            memory_parts = []
            
            for memory, similarity in relevant_memories[:5]:  # åªä½¿ç”¨æœ€ç›¸å…³çš„5æ¡è®°å¿†
                memory_text = f"{memory.content} (ç›¸å…³åº¦: {similarity:.2f})"
                memory_parts.append(memory_text)
            
            return "ç›¸å…³è®°å¿†: " + "; ".join(memory_parts)
            
        except Exception as e:
            logger.error(f"æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return ""
    
    async def _generate_response(
        self,
        messages: List[Dict[str, str]],
        character_config: Dict[str, Any],
        metadata: Dict[str, Any]
    ) -> str:
        """ç”ŸæˆAIå“åº”ï¼ˆéæµå¼ï¼‰"""
        try:
            # ç¡®ä¿OllamaæœåŠ¡å·²åˆå§‹åŒ–
            if not self.ollama_service:
                await self._init_ai_clients()
            
            # é…ç½®ç”Ÿæˆå‚æ•°
            model_config = settings.ai.model
            provider = model_config.provider
            
            # æ ¹æ®æä¾›å•†é€‰æ‹©ä¸åŒçš„ç”Ÿæˆæ–¹æ³•
            if provider == "ollama" and self.ollama_service:
                # ä½¿ç”¨Ollamaç”Ÿæˆ
                model_name = model_config.name
                
                # å¦‚æœæŒ‡å®šçš„æ¨¡å‹ä¸å¯ç”¨ï¼Œå°è¯•æ¨èä¸€ä¸ª
                if not self.ollama_service.is_model_available(model_name):
                    recommended_model = await self.ollama_service.suggest_model_for_task("chat")
                    if recommended_model:
                        model_name = recommended_model
                        logger.info(f"ä½¿ç”¨æ¨èæ¨¡å‹: {model_name}")
                    else:
                        return "æŠ±æ­‰ï¼Œæ²¡æœ‰å¯ç”¨çš„Ollamaæ¨¡å‹ã€‚"
                
                generated_text = await self.ollama_service.generate_response(
                    model=model_name,
                    messages=messages,
                    temperature=model_config.temperature,
                    max_tokens=model_config.max_tokens,
                    top_p=model_config.top_p
                )
                
                # æ›´æ–°å…ƒæ•°æ®
                metadata["model_used"] = model_name
                metadata["provider"] = "ollama"
                metadata["tokens_used"] += len(generated_text.split())  # ä¼°ç®—tokenæ•°
                
                return generated_text or "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›å¤ã€‚"
                
            elif provider == "openai" and self.openai_client:
                # ä½¿ç”¨OpenAIç”Ÿæˆ
                response = await self.openai_client.chat.completions.create(
                    model=model_config.name,
                    messages=messages,
                    temperature=model_config.temperature,
                    max_tokens=model_config.max_tokens,
                    top_p=model_config.top_p,
                    frequency_penalty=model_config.frequency_penalty,
                    presence_penalty=model_config.presence_penalty
                )
                
                generated_text = response.choices[0].message.content
                
                # æ›´æ–°å…ƒæ•°æ®
                metadata["model_used"] = model_config.name
                metadata["provider"] = "openai"
                metadata["tokens_used"] += response.usage.total_tokens
                
                return generated_text or "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›å¤ã€‚"
            
            else:
                # å›é€€åˆ°é»˜è®¤é”™è¯¯ä¿¡æ¯
                available_providers = []
                if self.ollama_service and self.ollama_service.available_models:
                    available_providers.append("ollama")
                if self.openai_client:
                    available_providers.append("openai")
                
                if available_providers:
                    return f"æŠ±æ­‰ï¼Œé…ç½®çš„AIæä¾›å•† '{provider}' ä¸å¯ç”¨ã€‚å¯ç”¨çš„æä¾›å•†: {', '.join(available_providers)}"
                else:
                    return "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
            
        except Exception as e:
            logger.error(f"ç”ŸæˆAIå“åº”å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜: {str(e)}"
    
    async def _generate_streaming_response(
        self,
        messages: List[Dict[str, str]],
        character_config: Dict[str, Any],
        stream_id: str,
        metadata: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆæµå¼AIå“åº”"""
        try:
            # ç¡®ä¿OllamaæœåŠ¡å·²åˆå§‹åŒ–
            if not self.ollama_service:
                await self._init_ai_clients()
            
            # é…ç½®ç”Ÿæˆå‚æ•°
            model_config = settings.ai.model
            provider = model_config.provider
            
            # å‘é€å¼€å§‹ä¿¡å·
            await self.stream_handler.send_status_update(
                stream_id, "generating", "AIæ­£åœ¨æ€è€ƒä¸­..."
            )
            
            # æ ¹æ®æä¾›å•†é€‰æ‹©ä¸åŒçš„æµå¼ç”Ÿæˆæ–¹æ³•
            if provider == "ollama" and self.ollama_service:
                # ä½¿ç”¨Ollamaæµå¼ç”Ÿæˆ
                model_name = model_config.name
                
                # å¦‚æœæŒ‡å®šçš„æ¨¡å‹ä¸å¯ç”¨ï¼Œå°è¯•æ¨èä¸€ä¸ª
                if not self.ollama_service.is_model_available(model_name):
                    recommended_model = await self.ollama_service.suggest_model_for_task("chat")
                    if recommended_model:
                        model_name = recommended_model
                        logger.info(f"ä½¿ç”¨æ¨èæ¨¡å‹: {model_name}")
                    else:
                        error_msg = "æŠ±æ­‰ï¼Œæ²¡æœ‰å¯ç”¨çš„Ollamaæ¨¡å‹ã€‚"
                        await self.stream_handler.send_stream_chunk(
                            stream_id, error_msg, is_final=True
                        )
                        return error_msg
                
                # å¤„ç†æµå¼å“åº”
                full_response = ""
                
                async def ollama_response_generator():
                    nonlocal full_response
                    async for token in self.ollama_service.generate_streaming_response(
                        model=model_name,
                        messages=messages,
                        temperature=model_config.temperature,
                        max_tokens=model_config.max_tokens,
                        top_p=model_config.top_p
                    ):
                        full_response += token
                        yield token
                
                # ä½¿ç”¨æµå¤„ç†å™¨ä¼ è¾“å“åº”
                await self.stream_handler.stream_ai_response(
                    stream_id, ollama_response_generator(), character_config
                )
                
                # æ›´æ–°å…ƒæ•°æ®
                metadata["model_used"] = model_name
                metadata["provider"] = "ollama"
                metadata["tokens_used"] += len(full_response.split())  # ä¼°ç®—tokenæ•°
                
                return full_response or "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›å¤ã€‚"
                
            elif provider == "openai" and self.openai_client:
                # ä½¿ç”¨OpenAIæµå¼ç”Ÿæˆ
                # åˆ›å»ºæµå¼å“åº”
                stream = await self.openai_client.chat.completions.create(
                    model=model_config.name,
                    messages=messages,
                    temperature=model_config.temperature,
                    max_tokens=model_config.max_tokens,
                    top_p=model_config.top_p,
                    frequency_penalty=model_config.frequency_penalty,
                    presence_penalty=model_config.presence_penalty,
                    stream=True
                )
                
                # å¤„ç†æµå¼å“åº”
                full_response = ""
                
                async def openai_response_generator():
                    nonlocal full_response
                    async for chunk in stream:
                        if chunk.choices[0].delta.content:
                            token = chunk.choices[0].delta.content
                            full_response += token
                            yield token
                
                # ä½¿ç”¨æµå¤„ç†å™¨ä¼ è¾“å“åº”
                await self.stream_handler.stream_ai_response(
                    stream_id, openai_response_generator(), character_config
                )
                
                # æ›´æ–°å…ƒæ•°æ®
                metadata["model_used"] = model_config.name
                metadata["provider"] = "openai"
                metadata["tokens_used"] += len(full_response.split())  # ç®€å•ä¼°ç®—
                
                return full_response or "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›å¤ã€‚"
            
            else:
                # å›é€€åˆ°é”™è¯¯ä¿¡æ¯
                available_providers = []
                if self.ollama_service and self.ollama_service.available_models:
                    available_providers.append("ollama")
                if self.openai_client:
                    available_providers.append("openai")
                
                if available_providers:
                    error_msg = f"æŠ±æ­‰ï¼Œé…ç½®çš„AIæä¾›å•† '{provider}' ä¸å¯ç”¨ã€‚å¯ç”¨çš„æä¾›å•†: {', '.join(available_providers)}"
                else:
                    error_msg = "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
                
                await self.stream_handler.send_stream_chunk(
                    stream_id, error_msg, is_final=True
                )
                return error_msg
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæµå¼AIå“åº”å¤±è´¥: {e}")
            error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜: {str(e)}"
            
            await self.stream_handler.send_stream_chunk(
                stream_id, error_msg, is_final=True
            )
            
            return error_msg
    
    async def _postprocess_response(
        self,
        response: str,
        character_config: Dict[str, Any],
        intent_analysis: Dict[str, Any]
    ) -> str:
        """åå¤„ç†ä¸ä¼˜åŒ–"""
        try:
            processed_response = response
            
            # 1. æ•æ„Ÿå†…å®¹è¿‡æ»¤
            processed_response = await self._filter_sensitive_content(processed_response)
            
            # 2. æ ¼å¼åŒ–å¤„ç†
            processed_response = self._format_response(processed_response)
            
            # 3. æ ¹æ®æ„å›¾è°ƒæ•´å›å¤
            processed_response = await self._adjust_response_for_intent(
                processed_response, intent_analysis, character_config
            )
            
            # 4. é•¿åº¦æ£€æŸ¥
            max_length = character_config.get("constraints", {}).get("response_length", {}).get("max", 1000)
            if len(processed_response) > max_length:
                processed_response = processed_response[:max_length-3] + "..."
            
            return processed_response
            
        except Exception as e:
            logger.error(f"åå¤„ç†å“åº”å¤±è´¥: {e}")
            return response
    
    async def _filter_sensitive_content(self, response: str) -> str:
        """è¿‡æ»¤æ•æ„Ÿå†…å®¹"""
        try:
            # ç®€å•çš„æ•æ„Ÿè¯è¿‡æ»¤
            sensitive_words = [
                "æ”¿æ²»", "æš´åŠ›", "è‰²æƒ…", "èµŒåš", "æ¯’å“",
                # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šæ•æ„Ÿè¯
            ]
            
            filtered_response = response
            for word in sensitive_words:
                if word in filtered_response:
                    filtered_response = filtered_response.replace(word, "*" * len(word))
            
            return filtered_response
            
        except Exception as e:
            logger.error(f"æ•æ„Ÿå†…å®¹è¿‡æ»¤å¤±è´¥: {e}")
            return response
    
    def _format_response(self, response: str) -> str:
        """æ ¼å¼åŒ–å›å¤"""
        try:
            # 1. å»é™¤å¤šä½™ç©ºç™½
            formatted = re.sub(r'\s+', ' ', response.strip())
            
            # 2. ç¡®ä¿å¥å­ç»“å°¾æœ‰æ ‡ç‚¹
            if formatted and not formatted[-1] in '.!?ã€‚ï¼ï¼Ÿ':
                formatted += 'ã€‚'
            
            # 3. ä¿®æ­£å¸¸è§çš„æ ¼å¼é—®é¢˜
            formatted = re.sub(r'\s+([,.!?ã€‚ï¼Œï¼ï¼Ÿ])', r'\1', formatted)  # æ ‡ç‚¹å‰ä¸åº”æœ‰ç©ºæ ¼
            formatted = re.sub(r'([,.!?ã€‚ï¼Œï¼ï¼Ÿ])\s*([a-zA-Z\u4e00-\u9fff])', r'\1 \2', formatted)  # æ ‡ç‚¹ååº”æœ‰ç©ºæ ¼
            
            return formatted
            
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–å›å¤å¤±è´¥: {e}")
            return response
    
    async def _adjust_response_for_intent(
        self,
        response: str,
        intent_analysis: Dict[str, Any],
        character_config: Dict[str, Any]
    ) -> str:
        """æ ¹æ®æ„å›¾è°ƒæ•´å›å¤"""
        try:
            intent = intent_analysis.get("primary_intent", "unknown")
            sentiment = intent_analysis.get("sentiment", "neutral")
            
            # æ ¹æ®æ„å›¾æ·»åŠ é€‚å½“çš„å¼€å¤´æˆ–ç»“å°¾
            if intent == "greeting":
                if not any(greeting in response.lower() for greeting in ["ä½ å¥½", "hello", "hi"]):
                    response = "ä½ å¥½ï¼" + response
            
            elif intent == "farewell":
                if not any(farewell in response.lower() for farewell in ["å†è§", "bye", "goodbye"]):
                    response = response + " å†è§ï¼"
            
            elif intent == "question" and not response.strip().endswith(('ã€‚', '.', 'ï¼', '!')):
                response += "ã€‚"
            
            # æ ¹æ®æƒ…æ„Ÿè°ƒæ•´è¯­è°ƒ
            if sentiment == "negative":
                # å¯¹äºè´Ÿé¢æƒ…æ„Ÿï¼Œä½¿ç”¨æ›´æ¸©å’Œã€åŒç†å¿ƒçš„è¯­è°ƒ
                if not any(empathy in response.lower() for empathy in ["ç†è§£", "æŠ±æ­‰", "é—æ†¾"]):
                    response = "æˆ‘ç†è§£æ‚¨çš„æ„Ÿå—ã€‚" + response
            
            elif sentiment == "positive":
                # å¯¹äºæ­£é¢æƒ…æ„Ÿï¼Œå¯ä»¥æ›´çƒ­æƒ…ä¸€äº›
                character_traits = character_config.get("personality", {}).get("traits", [])
                if "çƒ­æƒ…" in character_traits or "æ´»æ³¼" in character_traits:
                    if not response.endswith(('!', 'ï¼')):
                        response = response.rstrip('ã€‚.') + 'ï¼'
            
            return response
            
        except Exception as e:
            logger.error(f"æ ¹æ®æ„å›¾è°ƒæ•´å›å¤å¤±è´¥: {e}")
            return response
    
    async def _update_memories(
        self,
        user_id: UUID,
        session_id: UUID,
        user_input: str,
        ai_response: str,
        context: ConversationContext
    ) -> None:
        """æ›´æ–°è®°å¿†"""
        try:
            # åˆ›å»ºå½“å‰å¯¹è¯çš„æ¶ˆæ¯åˆ—è¡¨
            current_messages = [
                ChatMessage(
                    id=UUID('00000000-0000-0000-0000-000000000000'),
                    session_id=session_id,
                    role=MessageRole.USER,
                    content=user_input
                ),
                ChatMessage(
                    id=UUID('00000000-0000-0000-0000-000000000001'),
                    session_id=session_id,
                    role=MessageRole.ASSISTANT,
                    content=ai_response
                )
            ]
            
            # ä»å¯¹è¯ä¸­æå–è®°å¿†
            new_memories = await self.memory_manager.extract_memories_from_conversation(
                user_id=user_id,
                session_id=session_id,
                messages=context.messages + current_messages
            )
            
            # å­˜å‚¨æ–°è®°å¿†
            for memory in new_memories:
                await self.memory_manager.store_memory(memory)
            
            logger.info(f"æ›´æ–°äº† {len(new_memories)} æ¡è®°å¿†")
            
        except Exception as e:
            logger.error(f"æ›´æ–°è®°å¿†å¤±è´¥: {e}")
    
    async def _generate_error_response(
        self,
        error: str,
        character_config: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆé”™è¯¯å¤„ç†å“åº”"""
        try:
            character_name = character_config.get("name", "AIåŠ©æ‰‹")
            
            # æ ¹æ®è§’è‰²ç‰¹å¾ç”Ÿæˆä¸åŒé£æ ¼çš„é”™è¯¯å“åº”
            personality_traits = character_config.get("personality", {}).get("traits", [])
            
            if "å¯çˆ±" in personality_traits:
                return f"å“å‘€ï¼Œ{character_name}é‡åˆ°äº†ä¸€ç‚¹å°é—®é¢˜å‘¢ ğŸ˜… è¯·ç¨åå†è¯•è¯•å§ï¼"
            elif "ä¸“ä¸š" in personality_traits:
                return f"æŠ±æ­‰ï¼Œç³»ç»Ÿå½“å‰é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚å¦‚é—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚"
            else:
                return f"æŠ±æ­‰ï¼Œ{character_name}æš‚æ—¶æ— æ³•å›å¤æ‚¨çš„æ¶ˆæ¯ï¼Œè¯·ç¨åå†è¯•ã€‚"
                
        except Exception:
            return "æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°äº†é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            health_status = {
                "status": "healthy",
                "timestamp": datetime.utcnow().isoformat(),
                "components": {}
            }
            
            # æ£€æŸ¥OllamaæœåŠ¡
            if not self.ollama_service:
                await self._init_ai_clients()
            
            if self.ollama_service:
                ollama_health = await self.ollama_service.health_check()
                health_status["components"]["ollama"] = ollama_health
                if ollama_health["status"] != "healthy":
                    health_status["status"] = "degraded"
            else:
                health_status["components"]["ollama"] = "not_configured"
            
            # æ£€æŸ¥OpenAIå®¢æˆ·ç«¯
            if self.openai_client:
                try:
                    # ç®€å•çš„APIæµ‹è¯•
                    test_response = await self.openai_client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[{"role": "user", "content": "test"}],
                        max_tokens=1
                    )
                    health_status["components"]["openai"] = "healthy"
                except Exception as e:
                    health_status["components"]["openai"] = f"error: {str(e)}"
                    health_status["status"] = "degraded"
            else:
                health_status["components"]["openai"] = "not_configured"
            
            # æ£€æŸ¥å„ä¸ªç®¡ç†å™¨
            health_status["components"]["context_manager"] = "healthy"
            health_status["components"]["memory_manager"] = "healthy"
            health_status["components"]["character_system"] = "healthy"
            health_status["components"]["stream_handler"] = "healthy"
            
            return health_status
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }


# å…¨å±€AIæµæ°´çº¿å®ä¾‹
ai_pipeline = AIPipeline()
